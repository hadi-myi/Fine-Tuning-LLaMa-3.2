{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c40ceaa-0e95-42dd-9f23-d0af5dc4a2aa",
   "metadata": {},
   "source": [
    "# Finetuning Llama 3.2 1B on English - Spanish Translations\n",
    "This notebook demonstrates how I fine-tuned a 1B parameter LLaMA 3.2 model using the [Unsloth](https://github.com/unslothai/unsloth) library for efficient fine tuning. The goal is to teach the model how to generate accurate spanish translations from an english sentence, using natural-language problem descriptions as input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35577107-0ee7-4e46-a99f-ecd77244890a",
   "metadata": {},
   "source": [
    "adapted from: \n",
    "- https://www.youtube.com/watch?v=YZW3pkIR-YE&t=505s\n",
    "\n",
    "other sources:\n",
    "- https://www.youtube.com/watch?v=bZcKYiwtw1I&t=572s\n",
    "- https://huggingface.co/docs/trl/en/sft_trainer#format-your-input-prompts\n",
    "- https://stackoverflow.com/questions/1663807/how-do-i-iterate-through-two-lists-in-parallel\n",
    "- https://discuss.huggingface.co/t/guide-the-best-way-to-calculate-the-perplexity-of-fixed-length-models/193/2\n",
    "- https://stackoverflow.com/questions/59209086/calculate-perplexity-in-pytorch\n",
    "- https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html\n",
    "- https://en.wikipedia.org/wiki/Perplexity\n",
    "- https://huggingface.co/docs/transformers/perplexity\n",
    "- https://huggingface.co/spaces/evaluate-metric/bleu\n",
    "- https://towardsdatascience.com/foundations-of-nlp-explained-bleu-score-and-wer-metrics-1a5ba06d812b/\n",
    "- https://cloud.google.com/translate/docs/advanced/automl-evaluate\n",
    "\n",
    "\n",
    "Dataset used: \n",
    "- https://huggingface.co/datasets/hadi-myi2/TatoebaEN-ES\n",
    "- This data is originally from https://tatoeba.org/en/, I downloaded english-spanish sentence pairs from here and uploaded them to huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2421eaa9-abc2-486b-8a0f-832572f6c27b",
   "metadata": {},
   "source": [
    "All imports and libraries needed for this Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b94202f-8aeb-4fa4-977e-03a4a931ddc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "from datasets import load_dataset\n",
    "from transformers import TextStreamer\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b5182c-bc11-41ca-8e62-b833244d5663",
   "metadata": {},
   "source": [
    "# Load Base Llama 3.2 1 B Model\n",
    "- use 4 bit quantization. This greatly reduces GPU memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97bdb68b-4aa1-4bf6-abdf-79b6ddcc6e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    GRID A100X-20C. Num GPUs = 1. Max memory: 19.996 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "max_seq_length = 2048 \n",
    "dtype = None # None for auto detection.\n",
    "load_in_4bit = True # Use 4bit quantization \n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-1B-Instruct\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abddf95b-6733-4578-8e9b-e7233be0a3c5",
   "metadata": {},
   "source": [
    "# Applying Low Rank Adaptation\n",
    "- updates only a small number of parameters in specific layers, uses gradient checkpointing. This makes the model lightweight enough for limited hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52282384-15f9-4c41-b4f7-06d8e437c327",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, \n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, \n",
    "    bias = \"none\",    \n",
    "    use_gradient_checkpointing = \"unsloth\", \n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  \n",
    "    loftq_config = None, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9fd198-54ba-4033-b88c-299a38a50f88",
   "metadata": {},
   "source": [
    "# Formatting the Data\n",
    "- This prepares the dataset to be used for fine tuning by converting it into a format Llama 3.2 is compatible with. \n",
    "- Use LLaMA 3.1-style chat template for formatting prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51c74ed4-de0d-4e4c-bf62-5b9f4491833e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 244206/244206 [00:10<00:00, 23004.06 examples/s]\n",
      "Map: 100%|██████████| 27134/27134 [00:01<00:00, 22635.40 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(example):\n",
    "    prompts = []\n",
    "    \n",
    "    for english, spanish in zip(example[\"EN\"], example[\"ES\"]):\n",
    "        convo = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"{english.strip()}\\n\\n Translate the english content above to spanish.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": spanish.strip(),\n",
    "            }\n",
    "        ]\n",
    "        prompt_text = tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False)\n",
    "        prompts.append(prompt_text)\n",
    "    \n",
    "    return { \"text\": prompts }\n",
    "\n",
    "\n",
    "\n",
    "# load the full dataset\n",
    "full_dataset = load_dataset(\"hadi-myi2/TatoebaEN-ES\", split=\"train\")\n",
    "\n",
    "# split into train and test (90% train 10% test)\n",
    "dataset_split = full_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "train_dataset = dataset_split[\"train\"]\n",
    "test_dataset = dataset_split[\"test\"]\n",
    "\n",
    "# preprocess train set\n",
    "train_dataset = standardize_sharegpt(train_dataset)\n",
    "train_dataset = train_dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "# preprocess test set\n",
    "test_dataset = standardize_sharegpt(test_dataset)\n",
    "test_dataset = test_dataset.map(formatting_prompts_func, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f6e213-7a69-4370-a250-18b29fa7dcd8",
   "metadata": {},
   "source": [
    "Check how the data turned out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0f2ec028-7d45-4f52-97ea-e8b07ae90864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 July 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Verga is a famous writer.\n",
      "\n",
      " Translate the english content above to spanish.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Verga es un escritor famoso.<|eot_id|>\n",
      "eoe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nToday is the day of my predestined meeting.\\n\\n Translate the english content above to spanish.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nHoy es el día de mi cita predestinada.<|eot_id|>'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(test_dataset[5]['text'])\n",
    "print(\"eoe\")\n",
    "train_dataset[5]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9942afe-f090-4d61-89b0-68768df38556",
   "metadata": {},
   "source": [
    "# Initialize SFTTrainer\n",
    "\n",
    "- This block contains the training arguments for our finetuning.\n",
    "This configuration uses:\n",
    "- A batch size of 2, with gradient accumulation of 4 \n",
    "- Linear learning rate scheduler with a learning rate of `2e-4`\n",
    "- 8-bit AdamW optimizer to reduce memory usage\n",
    "- Mixed-precision training (FP16 or BF16 depending on hardware support)\n",
    "- 5000 steps of training over the dataset ( different from the leetcode finetuning, because this dataset is much larger)\n",
    "\n",
    "Other options:\n",
    "- packing=False: disables packing multiple sequences together \n",
    "- dataset_text_field='text': the dataset contains pre-formatted input/output examples under the \"text\" key\n",
    "- output_dir=\"outputs\": directory for saving checkpoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f14bfff4-c7df-4f43-8fff-9b51865c7138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Tokenizing [\"text\"] (num_proc=2): 100%|██████████| 244206/244206 [00:22<00:00, 11095.04 examples/s]\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, \n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        #num_train_epochs = 1, \n",
    "        max_steps = 5000,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 100,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", \n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae9981c-4180-4b5f-aa32-1689def36d30",
   "metadata": {},
   "source": [
    "Make sure prompts look good after tokeinziation by decoding them. Should match the previous block where I printed the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61c4321a-b5eb-4d24-9102-e8c9aa37925a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nToday is the day of my predestined meeting.\\n\\n Translate the english content above to spanish.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nHoy es el día de mi cita predestinada.<|eot_id|>'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(trainer.train_dataset[5][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a3cd259-25b7-4732-9efa-ae3ceedf214c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = GRID A100X-20C. Max memory = 19.996 GB.\n",
      "2.914 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f7f355-4c8d-44e8-9a5c-62cd6e01eedc",
   "metadata": {},
   "source": [
    "# Start Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adc39431-ed2f-49dc-9428-d5789d58b97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 244,206 | Num Epochs = 1 | Total steps = 5,000\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 11,272,192/1,000,000,000 (1.13% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5000/5000 58:26, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.058000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.570300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.586000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.600300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.580400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.586500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.580300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.568200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.571600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.550500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.549400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.559600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.562600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.560200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.549400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.564200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.562300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.556200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.559200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.547000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.540300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.554200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.550700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.558500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.534300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.542000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.547300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.529300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.533800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.546600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.549300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.546700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.533600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.537100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.528300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.522700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.524500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.536400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.535800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.525500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.516000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.524200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.528500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.526800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.531800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.538600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.527000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.515000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.531600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.532100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9322c82-1d02-4e59-a8f1-4f414a7a8d61",
   "metadata": {},
   "source": [
    "# Inference on the Fintuned Model \n",
    "\n",
    "\n",
    "- After training, we can test the model by feeding it natural language prompts and observing its Spanish text generation. Below is an example where the model is asked to compute various translations of English text.\n",
    "\n",
    "- We use the llama-3.1 chat template to structure the prompt. The output is then decoded back into human readable text using the tokenizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ddea8aab-15ad-4e17-8848-82727059cd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTranslate this english text to spanish: 'I am a computer science major at Illinois Wesleyan University'<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nTal vez el texto siguiente en inglés: 'Soy un estudiante en ingeniería de informática de Illinois Wesleyan University'.<|eot_id|>\"]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model) \n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Translate this english text to spanish: 'I am a computer science major at Illinois Wesleyan University'\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, \n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True,\n",
    "                         temperature = 1.5, min_p = 0.1)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afad0c6-3403-4b29-a786-d581aacbf59b",
   "metadata": {},
   "source": [
    "more testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a28718ec-6613-4e03-afac-f87ddd6c7cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muriel es de 20 años ahora.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"\"\"\n",
    "    Translate this english text to spanish: Muriel is 20 years old now \n",
    "    \"\"\"},]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True,\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 512,\n",
    "                   use_cache = True, temperature = 0.7, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318794a7-61f2-4778-8fc9-e9f00848d188",
   "metadata": {},
   "source": [
    "# Evaluating Model Performance: Perplexity\n",
    "\n",
    "To measure how well our finetuned LLaMA model learned the LeetCode task, we calculate **perplexity**, a common metric for language modeling.\n",
    "\n",
    "Perplexity reflects how surprised the model is by the correct answer. The lower the score, the better the model's predictive ability.\n",
    "\n",
    "- Evaluates the model’s ability to predict the next set of tokens.\n",
    "- Lower values = better fluency and alignment with expected output\n",
    "- computed perplexity over 1000 examples using a sliding window (stride) approach\n",
    "Limitation: \n",
    "- Perplexity evaluation is only run on the training set, not a validation set. As a result, it measures training effectiveness, but not real-world performance.\n",
    "### Formula:\n",
    "\n",
    "**Perplexity = exp(−1/N * Σ log P(xᵢ))**\n",
    "Where:\n",
    "- **N** is the number of tokens\n",
    "- **P(xᵢ)** is the predicted probability of the *i-th* token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43450393-4b79-4efe-a955-a697a28c7b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:40<00:00, 24.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity: 1.710835576057434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# get max input length\n",
    "max_length = model.config.max_position_embeddings\n",
    "\n",
    "device = model.device\n",
    "\n",
    "# https://huggingface.co/docs/datasets/v1.2.0/processing.html used this to reference the shuffle() function\n",
    "shuffled_dataset = test_dataset.shuffle(seed = 42).select(range(1000)) # the range here is the number of samples we are testing.\n",
    "\n",
    "# accumulators to keep track of total negative log likelihood and total number of tokens used\n",
    "nll_sum = 0.0\n",
    "n_tokens = 0\n",
    "stride = 512\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# loop over each prompt\n",
    "# reference for explanation of code: https://huggingface.co/docs/transformers/perplexity\n",
    "for example in tqdm(shuffled_dataset):\n",
    "    prompt = example['text']\n",
    "    encodings = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = encodings.input_ids\n",
    "    seq_len = input_ids.size(1)\n",
    "\n",
    "    prev_end_loc = 0\n",
    "\n",
    "    for begin_loc in range(0, seq_len, stride):\n",
    "        end_loc = min(begin_loc + max_length, seq_len)\n",
    "        trg_len = end_loc - prev_end_loc\n",
    "        inputs = input_ids[:, begin_loc:end_loc].to(device)\n",
    "\n",
    "        targets = inputs.clone()\n",
    "        targets[:, :-trg_len] = -100\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs, labels = targets)\n",
    "            n_log = outputs.loss\n",
    "\n",
    "        num_valid_tokens = (targets != -100).sum().item() \n",
    "        num_loss_tokens = num_valid_tokens - inputs.size(0)\n",
    "        nll_sum += n_log * num_loss_tokens\n",
    "        n_tokens += num_loss_tokens\n",
    "\n",
    "        prev_end_loc = end_loc\n",
    "        if end_loc == seq_len:\n",
    "            break\n",
    "\n",
    "avg_nll = nll_sum / n_tokens  # average negative log-likelihood per token\n",
    "ppl = torch.exp(avg_nll)\n",
    "\n",
    "print(f\"perplexity: {ppl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a728a3-b50d-44f3-b5d7-7b9062a94d5a",
   "metadata": {},
   "source": [
    "# Analysis of perplexity\n",
    "- A perplexity value close to 1.0 indicates that the model is perfect. \n",
    "- This model achieved a perplexity of approximately **~1.7**, which indicates that the finetuning was successful.\n",
    "- This aligns with qualitative inspection of the generated code, which looks mostly syntactically and semantically valid, however, it hasn't been tested quantitatively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a378dc4-ceaf-48b5-88cc-d40f9d759773",
   "metadata": {},
   "source": [
    "# Save model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "65efcebf-b2d7-4d0c-ba2b-e179cd1cad61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('models/llama-enes-4bit/tokenizer_config.json',\n",
       " 'models/llama-enes-4bit/special_tokens_map.json',\n",
       " 'models/llama-enes-4bit/tokenizer.json')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model.save_pretrained(\"models/llama-enes-4bit\")\n",
    "tokenizer.save_pretrained(\"models/llama-enes-4bit\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a03821-9713-4bbb-bfd7-c7d2f99634ec",
   "metadata": {},
   "source": [
    "- install to run BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "148ec32f-30bb-450f-a6d4-7f8125264d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.3 environment at: /home/exouser/.venv\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 6ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f244c5df-fe76-4248-b59c-48d912927b3f",
   "metadata": {},
   "source": [
    "## Evaluation of Translation with BLEU\n",
    "\n",
    "To evaluate the quality of the model's translations, I used the BLEU score (Bilingual Evaluation Understudy). BLEU is a widely used metric in machine translation that measures how similar the model’s output is to one or more reference translations.\n",
    "\n",
    "### How BLEU Works:\n",
    "- It compares n-gram overlaps between the model's generated output and the reference text\n",
    "- A score of 1.0 means a perfect match; 0.0 means no overlap at all\n",
    "- BLEU penalizes short or incomplete translations with a brevity penalty\n",
    "\n",
    "### How I Used It:\n",
    "- I passed English sentences through the model to generate Spanish translations\n",
    "- Each output was compared against the Spanish version from the dataset\n",
    "- The BLEU score was computed using the Hugging Face evaluate library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f42d5c4-ba5f-4844-a567-3305385a96a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [05:29<00:00,  3.04it/s]\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "# load BLEU from HF's evaluate library\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "# initialize empty lists to store predictions, outputs from the model, and references, which are corresponding translations from the dataset\n",
    "predictions = []\n",
    "references = []\n",
    "# loop through 1000 random examples from the dataset\n",
    "for example in tqdm(shuffled_dataset):\n",
    "    # get english and spanish pairs\n",
    "    prompt = example[\"EN\"].strip()\n",
    "    reference_translation = example[\"ES\"].strip()\n",
    "\n",
    "    # format inputs so they can be fed in the model\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": f\"{prompt}\\n\\nTranslate the english content above to spanish.\"}],\n",
    "        tokenize=True,\n",
    "        add_generation_prompt= True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    # Run inference with the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs,\n",
    "            max_new_tokens=128,\n",
    "            use_cache=True,\n",
    "            temperature=0.7,\n",
    "            min_p=0.1,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    # decode the output to text\n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # fixed some issues relating to how the output was formatted\n",
    "    if \"assistant\" in decoded_output:\n",
    "        decoded_output = decoded_output.split(\"assistant\")[-1].strip()\n",
    "    # take only the first line, rest of it might be irrelevant. Most translations should be 1 sentence.\n",
    "    decoded_output = decoded_output.split(\"\\n\")[0].strip()\n",
    "\n",
    "    # store in a list to calculate BLEU later\n",
    "    predictions.append(decoded_output)\n",
    "    references.append([reference_translation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99c9a526-c0f2-4652-868b-95098995bbc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: {'bleu': 0.4211602070536959, 'precisions': [0.6927583198304873, 0.47629218282785135, 0.3554706956666113, 0.26824418373434084], 'brevity_penalty': 1.0, 'length_ratio': 1.0171146044624746, 'translation_length': 8023, 'reference_length': 7888}\n"
     ]
    }
   ],
   "source": [
    "# calculate BLEU score.\n",
    "results = bleu.compute(predictions=predictions, references=references)\n",
    "print(\"BLEU Score:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c70182-0ff7-4a3d-8e62-6ec0d9e4f4b0",
   "metadata": {},
   "source": [
    "# Analysis of BLEU Score\n",
    "\n",
    "The model achieved a BLEU score of **0.427**, which is a strong result given the simplicity of the finetuning setup. While scores around **0.6+** are typically considered near-perfect for machine translation, a 0.42 score suggests that the model produces outputs that are largely accurate and fluent.\n",
    "\n",
    "  \n",
    "- **Length Ratio:**  \n",
    "  - 1.01 — Generated translations are nearly identical in length to reference translations on average\n",
    "\n",
    "- **Brevity Penalty:**  \n",
    "  - 1.0 — Indicates that the model is not under-generating; translations are sufficiently complete\n",
    "\n",
    "### Interpretation:\n",
    "Together, these metrics indicate that the fine-tuning was successful, especially considering the limited scale of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0080f1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
