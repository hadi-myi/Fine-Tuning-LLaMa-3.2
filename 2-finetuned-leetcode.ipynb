{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f76e56a",
   "metadata": {},
   "source": [
    "# Finetuning LLaMA 3.2 1B on Python LeetCode Solutions\n",
    "\n",
    "This notebook demonstrates how I fine-tuned a 1B parameter LLaMA 3.2 model using the [Unsloth](https://github.com/unslothai/unsloth) library for efficient fine tuning. The goal is to teach the model how to generate clean Python solutions to LeetCode-style algorithm problems, using natural-language problem descriptions as input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35577107-0ee7-4e46-a99f-ecd77244890a",
   "metadata": {},
   "source": [
    "adapted from: \n",
    "- https://www.youtube.com/watch?v=YZW3pkIR-YE&t=505s\n",
    "\n",
    "other sources:\n",
    "- https://www.youtube.com/watch?v=bZcKYiwtw1I&t=572s\n",
    "- https://huggingface.co/docs/trl/en/sft_trainer#format-your-input-prompts\n",
    "- https://stackoverflow.com/questions/1663807/how-do-i-iterate-through-two-lists-in-parallel\n",
    "- https://discuss.huggingface.co/t/guide-the-best-way-to-calculate-the-perplexity-of-fixed-length-models/193/2\n",
    "- https://stackoverflow.com/questions/59209086/calculate-perplexity-in-pytorch\n",
    "- https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html\n",
    "- https://en.wikipedia.org/wiki/Perplexity\n",
    "- https://huggingface.co/docs/transformers/perplexity\n",
    "- https://huggingface.co/spaces/evaluate-metric/bleu\n",
    "- https://towardsdatascience.com/foundations-of-nlp-explained-bleu-score-and-wer-metrics-1a5ba06d812b/\n",
    "\n",
    "Dataset used: \n",
    "https://huggingface.co/datasets/LimYeri/LeetCode_Python_Solutions_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d71ac7",
   "metadata": {},
   "source": [
    "All imports and libraries needed for this Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b94202f-8aeb-4fa4-977e-03a4a931ddc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exouser/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "from datasets import load_dataset\n",
    "from transformers import TextStreamer\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b465b0",
   "metadata": {},
   "source": [
    "# Load Base Llama 3.2 1 B Model\n",
    "- use 4 bit quantization. This greatly reduces GPU memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97bdb68b-4aa1-4bf6-abdf-79b6ddcc6e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    GRID A100X-20C. Num GPUs = 1. Max memory: 19.996 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 2048 \n",
    "dtype = None # None for auto detection.\n",
    "load_in_4bit = True # Use 4bit quantization \n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-1B-Instruct\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e462d6",
   "metadata": {},
   "source": [
    "# Applying Low Rank Adaptation\n",
    "- updates only a small number of parameters in specific layers, uses gradient checkpointing. This makes the model lightweight enough for limited hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52282384-15f9-4c41-b4f7-06d8e437c327",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.3.19 patched 16 layers with 16 QKV layers, 16 O layers and 16 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8f4328",
   "metadata": {},
   "source": [
    "# Formatting the Data\n",
    "- This prepares the dataset to be used for fine tuning by converting it into a format Llama 3.2 is compatible with. \n",
    "- Use LLaMA 3.1-style chat template for formatting prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51c74ed4-de0d-4e4c-bf62-5b9f4491833e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(example):\n",
    "    # get the two parts of the dataset we care about - the question and the python function\n",
    "    question = example[\"question_content\"]\n",
    "    content = example[\"content\"]\n",
    "\n",
    "    # for finetuning to work correctly, we need Just the code - no inline comments or explanations\n",
    "    lines = content.strip().splitlines()\n",
    "    \n",
    "    code_lines = []\n",
    "    in_code_block = False\n",
    "    seen_function = False\n",
    "\n",
    "    for line in lines:\n",
    "        stripped = line.strip()\n",
    "\n",
    "        # See if we are in a code block\n",
    "        if stripped.startswith(\"```python\") or stripped.startswith(\"```\"):\n",
    "            if in_code_block:\n",
    "                in_code_block = False\n",
    "            else:\n",
    "                in_code_block = True\n",
    "            continue\n",
    "\n",
    "        if not in_code_block:\n",
    "            continue\n",
    "\n",
    "        # find out if it is the start of a function\n",
    "        if stripped.startswith(\"def \"):\n",
    "            seen_function = True\n",
    "\n",
    "        # now we are in a function block \n",
    "        if seen_function:\n",
    "            new_line = \"\"\n",
    "            # Track starting of strings, quote character(single/ddouble)\n",
    "            in_string = False\n",
    "            quote = None\n",
    "            # loop iver each character\n",
    "            for char in line:\n",
    "                # handle each string start and end\n",
    "                if char in {\"'\", '\"'}:\n",
    "                    if in_string and char == quote:\n",
    "                        in_string = False\n",
    "                    elif not in_string:\n",
    "                        in_string = True\n",
    "                        quote = char\n",
    "                # stop if we run into # for inline comments\n",
    "                if not in_string and char == \"#\":\n",
    "                    break\n",
    "                new_line += char\n",
    "            # append lines that are not empty\n",
    "            if new_line.strip():\n",
    "                code_lines.append(new_line.rstrip())\n",
    "    # cleaned code - join all lines\n",
    "    code = \"\\n\".join(code_lines).strip()\n",
    "    \n",
    "    # return none if not code\n",
    "    if not code:\n",
    "        return {\"text\": None}\n",
    "\n",
    "    # follow llama's chat template to format. \n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"{question.strip()}\\n\\nWrite a Python function to solve the above problem.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": code,\n",
    "            },\n",
    "        ],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False,\n",
    "    )\n",
    "\n",
    "    return {\"text\": prompt}\n",
    "\n",
    "\n",
    "# load the full dataset\n",
    "full_dataset = load_dataset(\"LimYeri/LeetCode_Python_Solutions_v2\", split=\"train\")\n",
    "\n",
    "# split into train and test (90% train 10% test)\n",
    "dataset_split = full_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "train_dataset = dataset_split[\"train\"]\n",
    "test_dataset = dataset_split[\"test\"]\n",
    "\n",
    "# preprocess train set\n",
    "train_dataset = standardize_sharegpt(train_dataset)\n",
    "train_dataset = train_dataset.map(formatting_prompts_func, batched=False)\n",
    "\n",
    "# preprocess test set\n",
    "test_dataset = standardize_sharegpt(test_dataset)\n",
    "test_dataset = test_dataset.map(formatting_prompts_func, batched=False)\n",
    "\n",
    "# function to filter out invalid samples\n",
    "def is_valid(example):\n",
    "    text = example.get(\"text\")\n",
    "    if not text:\n",
    "        return False\n",
    "    if not isinstance(text, str):\n",
    "        return False\n",
    "    if not text.strip():\n",
    "        return False\n",
    "    if isinstance(text, list):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# filter train and test sets\n",
    "train_dataset = train_dataset.filter(is_valid)\n",
    "test_dataset = test_dataset.filter(is_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee78d54",
   "metadata": {},
   "source": [
    "Double check Data before proceeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f2ec028-7d45-4f52-97ea-e8b07ae90864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example from training dataset:\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 July 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "You are given an integer array `nums` where the `ith` bag contains `nums[i]` balls. You are also given an integer `maxOperations`.\n",
      "\n",
      "You can perform the following operation at most `maxOperations` times:\n",
      "\n",
      "*   Take any bag of balls and divide it into two new bags with a **positive** number of balls.\n",
      "    *   For example, a bag of `5` balls can become two new bags of `1` and `4` balls, or two new bags of `2` and `3` balls.\n",
      "\n",
      "Your penalty is the **maximum** number of balls in a bag. You want to **minimize** your penalty after the operations.\n",
      "\n",
      "Return _the minimum possible penalty after performing the operations_.\n",
      "\n",
      "**Example 1:**\n",
      "\n",
      "**Input:** nums = \\[9\\], maxOperations = 2\n",
      "**Output:** 3\n",
      "**Explanation:** \n",
      "- Divide the bag with 9 balls into two bags of sizes 6 and 3. \\[**9**\\] -> \\[6,3\\].\n",
      "- Divide the bag with 6 balls into two bags of sizes 3 and 3. \\[**6**,3\\] -> \\[3,3,3\\].\n",
      "The bag with the most number of balls has 3 balls, so your penalty is 3 and you should return 3.\n",
      "\n",
      "**Example 2:**\n",
      "\n",
      "**Input:** nums = \\[2,4,8,2\\], maxOperations = 4\n",
      "**Output:** 2\n",
      "**Explanation:**\n",
      "- Divide the bag with 8 balls into two bags of sizes 4 and 4. \\[2,4,**8**,2\\] -> \\[2,4,4,4,2\\].\n",
      "- Divide the bag with 4 balls into two bags of sizes 2 and 2. \\[2,**4**,4,4,2\\] -> \\[2,2,2,4,4,2\\].\n",
      "- Divide the bag with 4 balls into two bags of sizes 2 and 2. \\[2,2,2,**4**,4,2\\] -> \\[2,2,2,2,2,4,2\\].\n",
      "- Divide the bag with 4 balls into two bags of sizes 2 and 2. \\[2,2,2,2,2,**4**,2\\] -> \\[2,2,2,2,2,2,2,2\\].\n",
      "The bag with the most number of balls has 2 balls, so your penalty is 2, and you should return 2.\n",
      "\n",
      "**Constraints:**\n",
      "\n",
      "*   `1 <= nums.length <= 105`\n",
      "*   `1 <= maxOperations, nums[i] <= 109`\n",
      "\n",
      "Write a Python function to solve the above problem.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "def minimumSize(self, nums: List[int], maxOperations: int) -> int:\n",
      "        n=len(nums);\n",
      "        def check(x,op):\n",
      "            for i in range(n):\n",
      "                op-=(nums[i]//x);\n",
      "                if(nums[i]%x==0):\n",
      "                    op+=1\n",
      "            return True if op>=0 else False;\n",
      "        start=1;\n",
      "        end=max(nums);\n",
      "        ans=-1;\n",
      "        while start<=end:\n",
      "            mid=(start+end)//2\n",
      "            if(check(mid,maxOperations)):\n",
      "                ans=mid;\n",
      "                end=mid-1\n",
      "            else:\n",
      "                start=mid+1;\n",
      "        return ans;<|eot_id|>\n",
      "Example from test dataset:\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 July 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Given a string `s`, find the length of the **longest** **substring** without repeating characters.\n",
      "\n",
      "**Example 1:**\n",
      "\n",
      "**Input:** s =  \"abcabcbb \"\n",
      "**Output:** 3\n",
      "**Explanation:** The answer is  \"abc \", with the length of 3.\n",
      "\n",
      "**Example 2:**\n",
      "\n",
      "**Input:** s =  \"bbbbb \"\n",
      "**Output:** 1\n",
      "**Explanation:** The answer is  \"b \", with the length of 1.\n",
      "\n",
      "**Example 3:**\n",
      "\n",
      "**Input:** s =  \"pwwkew \"\n",
      "**Output:** 3\n",
      "**Explanation:** The answer is  \"wke \", with the length of 3.\n",
      "Notice that the answer must be a substring,  \"pwke \" is a subsequence and not a substring.\n",
      "\n",
      "**Constraints:**\n",
      "\n",
      "*   `0 <= s.length <= 5 * 104`\n",
      "*   `s` consists of English letters, digits, symbols and spaces.\n",
      "\n",
      "Write a Python function to solve the above problem.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "def lengthOfLongestSubstring(self, s: str) -> int:\n",
      "        string = s\n",
      "        max_length = 0\n",
      "        seen_character = ''\n",
      "        for letter in string:\n",
      "            if letter not in seen_character:\n",
      "                seen_character += letter\n",
      "            else:\n",
      "                seen_character = seen_character[seen_character.index(letter) + 1:] + letter\n",
      "            max_length = max(max_length, len(seen_character))\n",
      "        return max_length<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(\"Example from training dataset:\")\n",
    "print(train_dataset[50][\"text\"])\n",
    "\n",
    "print(\"Example from test dataset:\")\n",
    "print(test_dataset[50][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b760d17",
   "metadata": {},
   "source": [
    "# Initialize SFTTrainer\n",
    "\n",
    "- This block contains the training arguments for our finetuning.\n",
    "This configuration uses:\n",
    "- A batch size of 2, with gradient accumulation of 4 \n",
    "- Linear learning rate scheduler with a learning rate of `2e-4`\n",
    "- 8-bit AdamW optimizer to reduce memory usage\n",
    "- Mixed-precision training (FP16 or BF16 depending on hardware support)\n",
    "- 3  epochs of training over the dataset\n",
    "\n",
    "Other options:\n",
    "- packing=False: disables packing multiple sequences together \n",
    "- dataset_text_field='text': the dataset contains pre-formatted input/output examples under the \"text\" key\n",
    "- output_dir=\"outputs\": directory for saving checkpoints\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f14bfff4-c7df-4f43-8fff-9b51865c7138",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, \n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        \n",
    "        num_train_epochs = 3, \n",
    "        #max_steps = 2000,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 100,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2169065a",
   "metadata": {},
   "source": [
    "Verify Dataset before starting training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61c4321a-b5eb-4d24-9102-e8c9aa37925a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nOur hero Teemo is attacking an enemy Ashe with poison attacks! When Teemo attacks Ashe, Ashe gets poisoned for a exactly `duration` seconds. More formally, an attack at second `t` will mean Ashe is poisoned during the **inclusive** time interval `[t, t + duration - 1]`. If Teemo attacks again **before** the poison effect ends, the timer for it is **reset**, and the poison effect will end `duration` seconds after the new attack.\\n\\nYou are given a **non-decreasing** integer array `timeSeries`, where `timeSeries[i]` denotes that Teemo attacks Ashe at second `timeSeries[i]`, and an integer `duration`.\\n\\nReturn _the **total** number of seconds that Ashe is poisoned_.\\n\\n**Example 1:**\\n\\n**Input:** timeSeries = \\\\[1,4\\\\], duration = 2\\n**Output:** 4\\n**Explanation:** Teemo's attacks on Ashe go as follows:\\n- At second 1, Teemo attacks, and Ashe is poisoned for seconds 1 and 2.\\n- At second 4, Teemo attacks, and Ashe is poisoned for seconds 4 and 5.\\nAshe is poisoned for seconds 1, 2, 4, and 5, which is 4 seconds in total.\\n\\n**Example 2:**\\n\\n**Input:** timeSeries = \\\\[1,2\\\\], duration = 2\\n**Output:** 3\\n**Explanation:** Teemo's attacks on Ashe go as follows:\\n- At second 1, Teemo attacks, and Ashe is poisoned for seconds 1 and 2.\\n- At second 2 however, Teemo attacks again and resets the poison timer. Ashe is poisoned for seconds 2 and 3.\\nAshe is poisoned for seconds 1, 2, and 3, which is 3 seconds in total.\\n\\n**Constraints:**\\n\\n*   `1 <= timeSeries.length <= 104`\\n*   `0 <= timeSeries[i], duration <= 107`\\n*   `timeSeries` is sorted in **non-decreasing** order.\\n\\nWrite a Python function to solve the above problem.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\ndef findPoisonedDuration(self, timeSeries: List[int], duration: int) -> int:\\n        if not timeSeries:\\n            return 0\\n        total = 0\\n        for i in range(len(timeSeries) - 1):\\n            total += min(duration, timeSeries[i + 1] - timeSeries[i])\\n        return total + duration<|eot_id|>\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(trainer.train_dataset[5][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a3cd259-25b7-4732-9efa-ae3ceedf214c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = GRID A100X-20C. Max memory = 19.996 GB.\n",
      "1.49 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73482fc5",
   "metadata": {},
   "source": [
    "# Start Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adc39431-ed2f-49dc-9428-d5789d58b97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 14,143 | Num Epochs = 3 | Total steps = 5,304\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 11,272,192/1,000,000,000 (1.13% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5304' max='5304' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5304/5304 58:46, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.868200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.671500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.633500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.599100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.559400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.551000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.492100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.489800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.469400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.442000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.437800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.414300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.393500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.397800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.382300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.357600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.352000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.323300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.280500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.286600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.274100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.254500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.258200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.247200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.248800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.248200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.242300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.247100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.235900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.233700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.226900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.235700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.223600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.208200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.198900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.180400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.175300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.163000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.172000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.176000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.161900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.174400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.167400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.166700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.161400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.158700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.164900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.155000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.173700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.157100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.158500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.161100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.158900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73110118",
   "metadata": {},
   "source": [
    "# Inference on the Fintuned Model \n",
    "\n",
    "\n",
    "- After training, we can test the model by feeding it natural language prompts and observing its Python code generation. Below is an example where the model is asked to solve various problems from LeetCode.\n",
    "\n",
    "- We use the llama-3.1 chat template to structure the prompt. The output is then decoded back into human readable text using the tokenizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddea8aab-15ad-4e17-8848-82727059cd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nsolve th 2sum problem leetcode,<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nYou are given four integers `num1`, `num2`, `minimum`, and `maximum`. Each of the integers `num1` and `num2` contains **exactly** `two` digits.\\n\\nReturn _the minimum sum that the two numbers can be added as integers to have_. Return `0`']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"solve th 2sum problem leetcode,\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True,\n",
    "                         temperature = 0.7, min_p = 0.1)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "853f1c41-eca1-46a0-809c-722cec5e79ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def twoSum(self, nums: List[int], target: int) -> List[int]:\n",
      "        for i in range(len(nums)):\n",
      "            for j in range(i+1,len(nums)):\n",
      "                if nums[i]+nums[j]==target:\n",
      "                    return [i,j]<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"\"\"\n",
    "    Given an array of integers nums and an integer target, return indices of the two numbers such that they add up to target.\n",
    "\n",
    "You may assume that each input would have exactly one solution, and you may not use the same element twice.\n",
    "\n",
    "You can return the answer in any order.\n",
    "    \"\"\"},]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 512,\n",
    "                   use_cache = True, temperature = 0.7, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c828460-c78a-4724-b9e9-f4f8694de42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def zigzagLevelOrder(self, root: Optional[TreeNode]) -> List[List[int]]:\n",
      "        ans = []\n",
      "        q = collections.deque()\n",
      "        q.append(root)\n",
      "        while q:\n",
      "            temp = []\n",
      "            for i in range(len(q)):\n",
      "                node = q.popleft()\n",
      "                if node.left: q.append(node.left)\n",
      "                if node.right: q.append(node.right)\n",
      "                temp.append(node.val)\n",
      "            ans.append(temp)\n",
      "        return ans<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"\"\"\n",
    "    Given the `root` of a binary tree, return _the zigzag level order traversal of its nodes' values_. (i.e., from left to right, then right to left for the next level and alternate between).\n",
    "    \"\"\"},]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 512,\n",
    "                   use_cache = True, temperature = 0.7, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37e692b",
   "metadata": {},
   "source": [
    "# Evaluating Model Performance: Perplexity\n",
    "\n",
    "To measure how well our finetuned LLaMA model learned the LeetCode task, we calculate **perplexity**, a common metric for language modeling.\n",
    "\n",
    "Perplexity reflects how surprised the model is by the correct answer. The lower the score, the better the model's predictive ability.\n",
    "\n",
    "- Evaluates the modelâ€™s ability to predict the next set of tokens.\n",
    "- Lower values = better fluency and alignment with expected output\n",
    "- computed perplexity over 1000 examples using a sliding window (stride) approach\n",
    "Limitation: \n",
    "- Perplexity evaluation is only run on the training set, not a validation set. As a result, it measures training effectiveness, but not real-world performance.\n",
    "### Formula:\n",
    "\n",
    "**Perplexity = exp(âˆ’1/N * Î£ log P(xáµ¢))**\n",
    "Where:\n",
    "- **N** is the number of tokens\n",
    "- **P(xáµ¢)** is the predicted probability of the *i-th* token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91d953d6-f1ce-4292-b925-892bf58648a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:46<00:00, 21.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity: 1.2483659982681274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# get max input length\n",
    "max_length = model.config.max_position_embeddings\n",
    "\n",
    "device = model.device\n",
    "\n",
    "# https://huggingface.co/docs/datasets/v1.2.0/processing.html used this to reference the shuffle() function\n",
    "shuffled_dataset = test_dataset.shuffle(seed = 42).select(range(1000)) # the range here is the number of samples we are testing.\n",
    "\n",
    "# accumulators to keep track of total negative log likelihood and total number of tokens used\n",
    "nll_sum = 0.0\n",
    "n_tokens = 0\n",
    "\n",
    "stride = 512\n",
    "model.eval()\n",
    "\n",
    "# loop over each prompt\n",
    "# reference for explanation of code: https://huggingface.co/docs/transformers/perplexity\n",
    "\n",
    "for example in tqdm(shuffled_dataset):\n",
    "    prompt = example['text']\n",
    "    encodings = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = encodings.input_ids\n",
    "    seq_len = input_ids.size(1)\n",
    "\n",
    "    prev_end_loc = 0\n",
    "\n",
    "    for begin_loc in range(0, seq_len, stride):\n",
    "        end_loc = min(begin_loc + max_length, seq_len)\n",
    "        trg_len = end_loc - prev_end_loc\n",
    "        inputs = input_ids[:, begin_loc:end_loc].to(device)\n",
    "\n",
    "        targets = inputs.clone()\n",
    "        targets[:, :-trg_len] = -100\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs, labels = targets)\n",
    "            n_log = outputs.loss\n",
    "\n",
    "        num_valid_tokens = (targets != -100).sum().item() \n",
    "        num_loss_tokens = num_valid_tokens - inputs.size(0)\n",
    "        nll_sum += n_log * num_loss_tokens\n",
    "        n_tokens += num_loss_tokens\n",
    "\n",
    "        prev_end_loc = end_loc\n",
    "        if end_loc == seq_len:\n",
    "            break\n",
    "\n",
    "avg_nll = nll_sum / n_tokens  # average negative log-likelihood per token\n",
    "ppl = torch.exp(avg_nll)\n",
    "\n",
    "print(f\"perplexity: {ppl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec3e262",
   "metadata": {},
   "source": [
    "# Analysis of perplexity\n",
    "- A perplexity value close to 1.0 indicates that the model is perfect. \n",
    "- This model achieved a perplexity of approximately **1.2**, which indicates that the finetuning was successful.\n",
    "- This aligns with qualitative inspection of the generated code, which looks mostly syntactically and semantically valid, however, it hasn't been tested quantitatively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1519c05d-fd72-413d-9846-ff5e47fe1d2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('models/llama-leetcode-4bit/tokenizer_config.json',\n",
       " 'models/llama-leetcode-4bit/special_tokens_map.json',\n",
       " 'models/llama-leetcode-4bit/tokenizer.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save Model locally.\n",
    "trainer.model.save_pretrained(\"models/llama-leetcode-4bit\")\n",
    "tokenizer.save_pretrained(\"models/llama-leetcode-4bit\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a1b238-ace1-417d-8f4c-445b6796c0d3",
   "metadata": {},
   "source": [
    "## Evaluation of Translation with BLEU\n",
    "\n",
    "To evaluate the quality of the model's translations, I used the BLEU score (Bilingual Evaluation Understudy). BLEU is a widely used metric in machine translation that measures how similar the modelâ€™s output is to one or more reference translations.\n",
    "\n",
    "### How BLEU Works:\n",
    "- It compares n-gram overlaps between the model's generated output and the reference text\n",
    "- A score of 1.0 means a perfect match; 0.0 means no overlap at all\n",
    "- BLEU penalizes short or incomplete translations with a brevity penalty\n",
    "- while not specifically for analyizing code, this metric is a good enough standard to measure how close generated code is to the examples.\n",
    "\n",
    "### How I Used It:\n",
    "- I passed Leetcode problems from my test set through the model to generate answers\n",
    "- Each output was compared against the solution from the dataset\n",
    "- The BLEU score was computed using the Hugging Face evaluate library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "133fc578-c768-4387-93f6-94275581551e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.3 environment at: /home/exouser/.venv\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 13ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3a187f22-ecca-407b-b0be-3fcd52667c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [18:17<00:00,  2.19s/it]\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "shuffled_dataset = test_dataset.shuffle(seed = 42).select(range(500)) # the range here is the number of samples we are testing.\n",
    "\n",
    "# Loop through examples\n",
    "for example in tqdm(shuffled_dataset):\n",
    "    prompt = example[\"question_content\"].strip()\n",
    "\n",
    "    text = example[\"text\"]\n",
    "    if \"<|start_header_id|>assistant<|end_header_id|>\" in text:\n",
    "        reference_code = text.split(\"<|start_header_id|>assistant<|end_header_id|>\")[1].split(\"<|eot_id|>\")[0].strip()\n",
    "    else:\n",
    "        reference_code = \"\"\n",
    "    \n",
    "    if not reference_code:\n",
    "        continue\n",
    "\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": f\"{prompt}\\n\\nWrite a Python function to solve the above problem.\"}],\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs,\n",
    "            max_new_tokens=512,\n",
    "            use_cache=True,\n",
    "            temperature=0.7, \n",
    "            min_p=0.1,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    # decode the output to text\n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # fixed some issues relating to how the output was formatted\n",
    "    if \"assistant\" in decoded_output:\n",
    "        decoded_output = decoded_output.split(\"assistant\")[-1].strip()\n",
    "    # take only the result of the function\n",
    "    decoded_output = decoded_output.strip()\n",
    "\n",
    "    predictions.append(decoded_output)\n",
    "    references.append([reference_code])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ac165f-f331-48f4-a51d-e8293c9bfd05",
   "metadata": {},
   "source": [
    "Print examples to double check everything is good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8ac63567-f311-4341-93ef-8109504f1786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def largest_merge(word1: str, word2: str) -> str:\n",
      "    merge = []\n",
      "    while word1 or word2:\n",
      "        if word1 > word2:\n",
      "            merge.append(word1[0])\n",
      "            word1 = word1[1:]\n",
      "        else:\n",
      "            merge.append(word2[0])\n",
      "            word2 = word2[1:]\n",
      "    return ''.join(merge)\n",
      "def areOccurrencesEqual(self, s: str) -> bool:\n",
      "        d={}\n",
      "        for i in s:\n",
      "            if i in d:\n",
      "                d[i]+=1\n",
      "            else:\n",
      "                d[i]=1\n",
      "        t=d[s[0]]\n",
      "        for v in d.values():\n",
      "            if v!=t:\n",
      "                return False\n",
      "        return True\n",
      "def is_sum_of_two_equal_to_three(nums):\n",
      "    count = 0\n",
      "    for i in range(len(nums)):\n",
      "        for j in range(i + 1, len(nums)):\n",
      "            if nums[i] + nums[j] == 3:\n",
      "                count += 1\n",
      "    return count == 1\n",
      "def areOccurrencesSame(s: str) -> bool:\n",
      "    char_count = {}\n",
      "    for c in s:\n",
      "        char_count[c] = char_count.get(c, 0) + 1\n",
      "    return all(x == y for x, y in char_count.items())\n"
     ]
    }
   ],
   "source": [
    "for reference in references[-2:]:\n",
    "    print(reference[0])\n",
    "\n",
    "for prediction in predictions[-2:]:\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "48714dbd-eb98-4bd5-825c-95cde34eafcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: {'bleu': 0.23789626033025443, 'precisions': [0.5693364135387745, 0.33044148833622516, 0.23164367840980446, 0.1782282249414443], 'brevity_penalty': 0.8013504751096447, 'length_ratio': 0.8186944752915047, 'translation_length': 47183, 'reference_length': 57632}\n"
     ]
    }
   ],
   "source": [
    "# calculate BLEU score.\n",
    "results = bleu.compute(predictions=predictions, references=references)\n",
    "print(\"BLEU Score:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1825ae-7148-4540-ada6-961d90cfcd6b",
   "metadata": {},
   "source": [
    "# Analysis of BLEU Score\n",
    "\n",
    "The model achieved a BLEU score of **0.237**, which, according to google's documentation of the BLEU function means \"The gist is clear, but has significant grammatical errors\". This means that the Fine tuning model manages to get the basic structure, syntax etc from training, but the solutions are mostly wrong or\n",
    "syntactically incorrect. \n",
    "\n",
    "  \n",
    "- **Length Ratio:**  \n",
    "  - 0/8 â€” Generated translations are nearly identical in length to reference solutions on average\n",
    "\n",
    "- **Brevity Penalty:**  \n",
    "  - 0.8 â€” Indicates that the model is under-generating; solutions generated aren't the correct length\n",
    "\n",
    "### Interpretation:\n",
    "I will have to evaluate BLEU scores on the base model with the same data to see if finetuning actually made a difference or not, which i will get into later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35656df-38b5-46e2-bb7a-4daf711114b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
